# 任务三

one-hot:  
这里的One-hot与数据挖掘任务中的操作是一致的，即将每一个单词使用一个离散的向量表示。具体将每个字/词编码一个索引，然后根据索引进行赋值。
One-hot表示方法的例子如下：


```python
sentence_1="我爱北京天安门"
sentence_2="我喜欢上海"
```

首先对所有句子的字进行索引，即将每个字确定一个编号,即可以得到：  
{  
	'我': 1, '爱': 2, '北': 3, '京': 4, '天': 5,  
  '安': 6, '门': 7, '喜': 8, '欢': 9, '上': 10, '海': 11  
}  
每个字对应一个只包含一个1其余皆为0的向量：  
我：[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  
爱：[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  
...  
海：[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  

词袋表示（word bags）,通过一个向量来记录一个句子中同每一个词汇出现的次数:  
句子1：我 爱 北 京 天 安 门  我 喜 欢 上 海  
转换为 [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  


```python
#$######借用sklearn库来实现bagging（count vectors）
####一共出现了九个单词，四个句子
from sklearn.feature_extraction.text import CountVectorizer
corpus = [
    'This is the first document.',
    'This document is the second document.',
    'And this is the third one.',
    'Is this the first document?',
]
vectorizer = CountVectorizer()
vectorizer.fit_transform(corpus).toarray()
```




    array([[0, 1, 1, 1, 0, 0, 1, 0, 1],
           [0, 2, 0, 1, 0, 1, 1, 0, 1],
           [1, 0, 0, 1, 1, 0, 1, 1, 1],
           [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)



N-gram将相邻单词组合成为新单词（更像记录词汇？？？）  
eg:N取值为二，bagging里的例子就变成如下形式  
句子1：我爱 爱北 北京 京天 天安 安门  
句子2：我喜 喜欢 欢上 上海  

# TF-IDF
TF-IDF 分数由两部分组成：第一部分是词语频率（Term Frequency）该词语在当前文档出现的次数 / 当前文档中词语的总数(调查单独文本)   
第二部分是逆文档频率  （Inverse Document Frequency）。log_e（文档总数 / 出现该词语的文档总数）（调查整体）  
其中计算语料库中文档总数除以含有该词语的文档数量，然 后再取对数就是逆文档频率。  


```python
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import RidgeClassifier###岭回归，是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法
from sklearn.metrics import f1_score###计算精确度
```


```python
train_df = pd.read_csv('D:/NLP/train_set.csv', sep='\t', nrows=15000)
```


```python
vectorizer = CountVectorizer(max_features=3000)####对所有关键词的term frequency进行降序排序，只取前max_features个作为关键词集
train_test = vectorizer.fit_transform(train_df['text'])####拟合模型，并返回文本矩阵
```


```python
clf = RidgeClassifier()
clf.fit(train_test[:10000], train_df['label'].values[:10000])#####利用训练集进行训练
```




    RidgeClassifier()




```python
val_pred = clf.predict(train_test[10000:])
print(f1_score(train_df['label'].values[10000:], val_pred, average='macro'))
```

    0.7410645311869299
    

TF-IDF + RidgeClassifier


```python
import pandas as pd

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import RidgeClassifier
from sklearn.metrics import f1_score


tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=3000)###提取特征
train_test = tfidf.fit_transform(train_df['text'])

clf = RidgeClassifier()
clf.fit(train_test[:10000], train_df['label'].values[:10000])

val_pred = clf.predict(train_test[10000:])
print(f1_score(train_df['label'].values[10000:], val_pred, average='macro'))
```

    0.8721598830546126
    


```python
import pandas as pd

from sklearn.feature_extraction.text import TfidfVectorizer####用adaboost
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import f1_score
from sklearn.tree import DecisionTreeClassifier

tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=3000)###提取特征
train_test = tfidf.fit_transform(train_df['text'])

clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5),n_estimators=40)
clf.fit(train_test[:10000], train_df['label'].values[:10000])

val_pred = clf.predict(train_test[10000:])
print(f1_score(train_df['label'].values[10000:], val_pred, average='macro'))
```

    0.5109290039406889
    


```python

```
