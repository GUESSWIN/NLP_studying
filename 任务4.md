```python
import fasttext
import torch
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.nn as F
```


```python
vocab_size = 2000####ci hui liang
embedding_dim = 100
max_word = 500
class_num = 5
```

# fasttext

fasttext优势  
（1）用单词的embedding叠加获得文档向量，将相似的句子分为一类  
（2）其学习到的embedding空间维度比较低，可以快速进行训练


```python
##### pytorch #####
class fast(nn.Module):
    def __init__(self,vocab_size,embedding_dim,max_word,class_num):
        super(fasttext,self).__init__()
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.max_word = max_word
        self.class_num = class_num
        
        self.embedding = nn.Embedding(vocab_size,embedding_dim,input_length=max_word)
        self.max_pool = nn.AdaptiveAvgPool1d()
        ####fully connected####
        self.fc = nn.Linear(embedding_dim,class_num)
        self.fc.weight.data.uniform(-0.03,0.03)
        
    def forward(self,x):
        x = self.embedding(x)
        x = self.max_pool(x)
        x = self.fc(x)
```

# 基于fasttext的文本分类


```python
train_df = pd.read_csv('/home/xxh/NLP/train_set.csv',sep='\t',nrows=15000)
```


```python
train_df['label_ft']='__label__'+train_df['label'].astype(str)
```


```python
train_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>text</th>
      <th>label_ft</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>2967 6758 339 2021 1854 3731 4109 3792 4149 15...</td>
      <td>__label__2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>11</td>
      <td>4464 486 6352 5619 2465 4802 1452 3137 5778 54...</td>
      <td>__label__11</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>7346 4068 5074 3747 5681 6093 1777 2226 7354 6...</td>
      <td>__label__3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>7159 948 4866 2109 5520 2490 211 3956 5520 549...</td>
      <td>__label__2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>3646 3055 3055 2490 4659 6065 3370 5814 2465 5...</td>
      <td>__label__3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>14995</th>
      <td>5</td>
      <td>1822 6040 5744 5310 4578 4407 6242 2313 3466 2...</td>
      <td>__label__5</td>
    </tr>
    <tr>
      <th>14996</th>
      <td>9</td>
      <td>88 7400 7539 4516 6122 290 6831 465 1647 6293 ...</td>
      <td>__label__9</td>
    </tr>
    <tr>
      <th>14997</th>
      <td>0</td>
      <td>2597 7160 2282 1407 4403 4516 2873 4597 7037 5...</td>
      <td>__label__0</td>
    </tr>
    <tr>
      <th>14998</th>
      <td>0</td>
      <td>2400 4411 4721 3289 5787 5096 4464 6250 1324 6...</td>
      <td>__label__0</td>
    </tr>
    <tr>
      <th>14999</th>
      <td>8</td>
      <td>4188 5778 5296 5640 2835 648 6122 2489 2923 39...</td>
      <td>__label__8</td>
    </tr>
  </tbody>
</table>
<p>15000 rows × 3 columns</p>
</div>




```python
train_df[['text','label_ft']].iloc[:-5000].to_csv('train.csv',index=None,header=None,sep='\t')
```


```python
model = fasttext.train_supervised('train.csv',lr=1.0,wordNgrams=2,verbose=2,minCount=1,epoch=25,loss='hs')
```


```python
val_pred=[model.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[-5000:]['text']]
```


```python
from sklearn.metrics import f1_score
```


```python
print(f1_score(train_df['label'].values[-5000:].astype(str),val_pred,average='macro'))
```

    0.8269601026960608


增加训练回合，效果不明显


```python
model_1 = fasttext.train_supervised('train.csv',lr=1.0,wordNgrams=2,verbose=2,minCount=1,epoch=100,loss='hs')
```


```python
val_pred_1=[model_1.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[-5000:]['text']]
```


```python
print(f1_score(train_df['label'].values[-5000:].astype(str),val_pred_1,average='macro'))
```

    0.8296678409119344


增加Ngram,预测反而下降


```python
model_2 = fasttext.train_supervised('train.csv',lr=1.0,wordNgrams=5,verbose=2,minCount=1,epoch=25,loss='hs')
val_pred_2=[model_2.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[-5000:]['text']]
print(f1_score(train_df['label'].values[-5000:].astype(str),val_pred_2,average='macro'))
```

    0.8078688211677213


改变loss，预测精度下降


```python
model_3 = fasttext.train_supervised('train.csv',lr=0.1,wordNgrams=2,verbose=2,minCount=1,epoch=25,loss='ova')
val_pred_3=[model_3.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[-5000:]['text']]
print(f1_score(train_df['label'].values[-5000:].astype(str),val_pred_3,average='macro'))
```

    0.5910854871989735



```python
model_4 = fasttext.train_supervised('train.csv',lr=1.0,wordNgrams=2,verbose=2,minCount=1,epoch=100,loss='ns')
val_pred_4=[model_4.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[-5000:]['text']]
print(f1_score(train_df['label'].values[-5000:].astype(str),val_pred_4,average='macro'))
```

    0.8768652134159024



```python

```
