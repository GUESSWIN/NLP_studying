# 任务5 基于深度学习的文本分类2

词向量（word2vec）：对出现在上下文环境里的词进行预测。对于每一个文本，对其选择一个上下窗口和一个中心词，  
然后基于中心词预测其他词汇的出现概率。
优势：可以从新增预料中学习到新增词的向量表达，高校的在线学习算法。

word2vec思路：通过单词和上下文彼此预测，分别对应：  
（1）skip-grams(SG):预测上下文  
（2）continuous bag of words(CBOW):预测目标单词  
同时有两种更为高效的训练方法：  
（1）Hierarchical softmax:分层softmax
（2）negative sampling:负采样

1、skip-grams原理： 
该算法通过给定的单词（输入）来预测上下文  
2、CBOW：
该算法通过给定的上下文来预测中心词汇  

word2vec分为两个部分，第一部分建立模型，第二部分通过模型获取嵌入词向量。  
word2vec首先通过训练数据构建神经网络，接着我们只需要利用网络曾中的参数即可，并不需要整个模型（为什么?参数不就代表了网络吗？）

skip-gram过程  
假设我们拥有一个句子（she is my girl friend）  
1\选择"my"作为input  
2\定义skip_windows参数，其代表从input的一侧所取词汇的量，若skip_windows=1，则序列为“is my girl friend”（my），  
另外一个参数叫num_skips，其代表从整个窗口中取多少个不同的词作为output，当skip_windo2,num_kips=2时，可以得到（in,out）:  
（“my”,"is"）,("my",girl)  
3\神经网路会根据输入给出输出的概率分布，如通过训练会告诉每一个单词“my”作为input时，其会被作为output的可能信息


模型的输出概率代表着词典中每个词有多大可能性和input同时出现。如Soviet和russia一起出现的概率会高于USA

2、skip-gram训练

从上文可以看出word2vec会是一个参数量巨大的模型，这对于训练来说不客观，因此提出解决方案：  
1\将常见的单词组合（word pairs）作为单个词来处理  
2\对高频词汇进行抽样来减少训练样本个数  
3\采用负采样，这可以让每个训练样本只会更新一部分的模型权重，从而降低计算负担

2.1词组（word pairs）和“phases”

例如“New YORK”这样的词汇就应该当作一个词汇进行bagging

2.2对高频词进行抽样

对于上下文会经常出现的词汇，其会造成以下影响：  
1\若num_skip比较大，距离中心词汇较远的经常出现的词汇对其语义判断没有更好的帮助  
2\文本中的高频词汇其出现的样本量远远高于所需求的样本量  
  
word2vec通过抽样方式来解决这个问题。其思想如下：对于训练文本中的每一个单词，它们都有一定的机率被删除，其被删除概率与出现评论有关  
  
wi代表单词，Z(wi)为其在语料中出现的频次，若wi在1000长度的语料中出现10词，Z(wi)=0.01  
P(wi)代表保留某个单词的概率：  
P（wi）=((Z(wi)/0.001)^0.5+1)(0.001/(X(wi)))

2.3负采样  
通过提高训练速度并且改善词向量的质量，负采样每次让一个训练样本仅更新一部分的权重。  
例如，我们的目标是（“dog”，“OW”）,若词典大小为10000时，输出层只希望对英语"OW"的神经节点输出1,其余节点都应该为0（负节点）   
负采样时，随即选择一小部分的负样本词汇更新对应的权重，同时也会对"positive"进行权重更新，出现频率越高的词汇越容易成为负采样词汇


```python

```
